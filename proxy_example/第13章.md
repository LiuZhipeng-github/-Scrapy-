## 第13章 使用 HTTP 代理
    执行 spider xici_proxy 时，要把 settings.py 中
    DOWNLOADER_MIDDLEWARES = {
    # 'proxy_example.middlewares.ProxyExampleDownloaderMiddleware': 543,
    #  随机代理下载中间件 2020-03-27
    'proxy_example.middlewares.RandomHttpProxyMiddleware': 745,
    #  测试代理中间件 2020-03-30
    #  'proxy_example.middlewares.RandomHttpProxyMiddleware': 746,
    #  默认代理中间件
    #  'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,
    } 注销掉。
    
### middlewares.py 
    
    # 实现随机代理 2020-03-27
    from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware
    from scrapy.exceptions import NotConfigured
    # 使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict
    from collections import defaultdict
    import json
    import random
    
    class RandomHttpProxyMiddleware(HttpProxyMiddleware):
    
        def __init__(self,auth_encoding='latin-1',proxy_list_file=None):
            if not proxy_list_file:
                # 当程序出现错误，python会自动引发异常，也可以通过raise显示地引发异常。一旦执行了raise语句，raise后面的语句将不能执行
                raise NotConfigured
    
            self.auth_encoding = auth_encoding
            # 分别用两个列表维护 HTTP 和 HTTPS 的代理 {'http':[...],'https':[...]}
            self.proxies = defaultdict(list)
    
            # 从json 文件中读取代理服务器信息，填入self.proxies
            with open (proxy_list_file) as f:
                proxy_list = json.load(f)
                for proxy in proxy_list:
                    scheme = proxy['proxy_scheme']
                    url = proxy['proxy']
                    # self.proxies[scheme].append(self._set_proxy(url,scheme))
                    self.proxies[scheme].append(self._get_proxy(url,scheme))
    
        # HttpProxyMiddleware原代码
        # def __init__(self, auth_encoding='latin-1'):
        #     self.auth_encoding = auth_encoding
        #     self.proxies = {}
        #     for type_, url in getproxies().items():
        #         self.proxies[type_] = self._get_proxy(url, type_)
    
    
        @classmethod
        def from_crawler(cls, crawler):
            # 从配置文件中读取用户验证信息的编码
            auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING','latain-1')
            # 从配置文件中读取代理文件服务器列表文件（json）的路径
            proxy_list_file = crawler.settings.get('HTTPPROXY_PROXY_LIST_FILE')
            return cls(auth_encoding,proxy_list_file)
    
        # HttpProxyMiddleware原代码
        # @classmethod
        # def from_crawler(cls, crawler):
        #     if not crawler.settings.getbool('HTTPPROXY_ENABLED'):
        #         raise NotConfigured
        #     auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')
        #     return cls(auth_encoding)
    
        def _set_proxy(self, request, scheme):
            # 随机选择一个代理
            a = self.proxies[scheme]
            creds,proxy = random.choice(self.proxies[scheme])
            request.meta['proxy'] = proxy
            if creds:
                request.headers['Proxy_Authorization'] = b'Basic' + creds
    
        # HttpProxyMiddleware原代码
        # def _set_proxy(self, request, scheme):
        #     creds, proxy = self.proxies[scheme]
        #     request.meta['proxy'] = proxy
        #     if creds:
        #         request.headers['Proxy-Authorization'] = b'Basic ' + creds
        
###  class HttpProxyMiddleware
    import base64
    from urllib.parse import unquote, urlunparse
    from urllib.request import getproxies, proxy_bypass, _parse_proxy
    
    from scrapy.exceptions import NotConfigured
    from scrapy.utils.httpobj import urlparse_cached
    from scrapy.utils.python import to_bytes
    
    
    class HttpProxyMiddleware(object):

    def __init__(self, auth_encoding='latin-1'):
        self.auth_encoding = auth_encoding
        self.proxies = {}
        for type_, url in getproxies().items():
            self.proxies[type_] = self._get_proxy(url, type_)

    @classmethod
    def from_crawler(cls, crawler):
        if not crawler.settings.getbool('HTTPPROXY_ENABLED'):
            raise NotConfigured
        auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')
        return cls(auth_encoding)

    def _basic_auth_header(self, username, password):
        user_pass = to_bytes(
            '%s:%s' % (unquote(username), unquote(password)),
            encoding=self.auth_encoding)
        return base64.b64encode(user_pass)

    def _get_proxy(self, url, orig_type):
        proxy_type, user, password, hostport = _parse_proxy(url)
        proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))

        if user:
            creds = self._basic_auth_header(user, password)
        else:
            creds = None

        return creds, proxy_url

    def process_request(self, request, spider):
        # ignore if proxy is already set
        if 'proxy' in request.meta:
            if request.meta['proxy'] is None:
                return
            # extract credentials if present
            creds, proxy_url = self._get_proxy(request.meta['proxy'], '')
            request.meta['proxy'] = proxy_url
            if creds and not request.headers.get('Proxy-Authorization'):
                request.headers['Proxy-Authorization'] = b'Basic ' + creds
            return
        elif not self.proxies:
            return

        parsed = urlparse_cached(request)
        scheme = parsed.scheme

        # 'no_proxy' is only supported by http schemes
        if scheme in ('http', 'https') and proxy_bypass(parsed.hostname):
            return

        if scheme in self.proxies:
            self._set_proxy(request, scheme)

    def _set_proxy(self, request, scheme):
        creds, proxy = self.proxies[scheme]
        request.meta['proxy'] = proxy
        if creds:
            request.headers['Proxy-Authorization'] = b'Basic ' + creds
    
    
    process_request（request，spider ）
    对于通过下载中间件的每个请求，都会调用此方法。
    
    process_request()应该是：return None，返回一个 Response对象，返回一个Request 对象或raise IgnoreRequest。
    
    如果返回None，则Scrapy将继续处理此请求，执行所有其他中间件，直到最终将适当的下载程序处理程序称为执行的请求（并下载了其响应）。
    
    如果它返回一个Response对象，Scrapy不会打扰调用任何其他process_request()或process_exception()方法，或相应的下载功能; 它将返回该响应。process_response() 总是在每个响应上调用已安装中间件的方法。
    
    如果返回一个Request对象，Scrapy将停止调用process_request方法并重新计划返回的请求。一旦执行了新返回的请求，就会在下载的响应上调用适当的中间件链。
    
    如果引发IgnoreRequest异常，则将process_exception()调用已安装的下载器中间件的 方法。如果它们都不处理该异常，Request.errback则调用请求（）的errback函数。如果没有代码处理引发的异常，则将其忽略并且不记录（与其他异常不同）。
    
    参数：	
    request（Request对象）–正在处理的请求
    蜘蛛（Spider对象）–此请求所针对的蜘蛛
    process_response（请求，响应，蜘蛛）
    process_response()应该是：返回一个Response 对象，返回一个Request对象或引发IgnoreRequest异常。
    
    如果返回一个Response（可能是相同的给定响应，或者是一个全新的响应），则该响应将继续使用process_response()链中下一个中间件的响应进行处理。
    
    如果它返回一个Request对象，则中间件链将暂停，并将返回的请求重新安排为将来下载。这与从中返回请求的行为相同process_request()。
    
    如果引发IgnoreRequest异常，Request.errback则调用请求（）的errback函数。如果没有代码处理引发的异常，则将其忽略并且不记录（与其他异常不同）。
    
    参数：	
    请求（是一个Request对象）–发起响应的请求
    响应（Response对象）–正在处理的响应
    蜘蛛（Spider对象）–此响应预期用于的蜘蛛
    process_exception（request，exception，spider ）
    process_exception()当下载处理程序或process_request()（从下载程序中间件中）引发异常（包括IgnoreRequest异常）时，Scrapy调用
    
    process_exception()应该返回：None，Response对象或Request对象。
    
    如果返回None，则Scrapy将继续处理此异常，并执行process_exception()已安装中间件的任何其他方法，直到没有剩余中间件并且默认异常处理开始为止。
    
    如果它返回一个Response对象，则将process_response() 启动已安装中间件的方法链，并且Scrapy不会费心调用任何其他process_exception()中间件方法。
    
    如果它返回一个Request对象，则将返回的请求重新安排为将来下载。这将停止执行 process_exception()中间件的方法，就像返回响应一样。
    
    参数：	
    请求（是一个Request对象）–生成异常的请求
    异常（Exception对象）–引发的异常
    蜘蛛（Spider对象）–此请求所针对的蜘蛛
    from_crawler（cls，爬虫）
    如果存在，则调用该类方法以从中创建中间件实例Crawler。它必须返回中间件的新实例。搜寻器对象提供对所有Scrapy核心组件（如设置和信号）的访问；它是中间件访问它们并将其功能连接到Scrapy中的一种方式。
    
    参数：	搜寻器（Crawler对象）–使用此中间件的搜寻器